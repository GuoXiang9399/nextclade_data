#!/usr/bin/env python3

"""
Builds a fresh data repo from source data
"""

import fnmatch
import json
import os
import shutil

from typing import List

THIS_DIR = os.path.dirname(os.path.realpath(__file__))
PROJECT_ROOT_DIR = os.path.realpath(os.path.join(THIS_DIR, ".."))

# Source data will be taken from this directory (relative to the project root)
# Committed to repository. Modify files there.
DATA_INPUT_DIR = os.path.realpath(os.path.join(PROJECT_ROOT_DIR, "data"))

# Final, uncompressed datasets will be generated in this directory (relative to the project root)
# It is gitignored and is safe to delete. Do not modify manually.
DATA_OUTPUT_DIR = os.path.realpath(os.path.join(PROJECT_ROOT_DIR, "data_output"))

SETTINGS_JSON_PATH = os.path.realpath(os.path.join(DATA_INPUT_DIR, "settings.json"))
INDEX_JSON_PATH = os.path.realpath(os.path.join(DATA_OUTPUT_DIR, "index.json"))

# In case we want to change these words
REFERENCE = "reference"
REFERENCES = "references"

from collections import namedtuple


def dict_to_namedtuple(name, dic):
    return namedtuple(name, dic.keys())(*dic.values())


def find_files(pattern, here):
    for path, dirs, files in os.walk(os.path.abspath(here)):
        for filename in fnmatch.filter(files, pattern):
            yield os.path.join(path, filename)


def find_dirs(here):
    for path, dirs, _ in os.walk(os.path.abspath(here)):
        for dirr in dirs:
            yield os.path.join(path, dirr)


def find_dirs_here(here):
    return filter(os.path.isdir, [os.path.join(here, e) for e in os.listdir(here)])


def json_write(obj, filepath):
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, "w") as f:
        json.dump(obj, f, indent=2, sort_keys=True)
        f.write("\n")


def get_paths(dataset_json, dataset_ref_json, version):
    dataset_name = dataset_json["name"]
    ref_accession = dataset_ref_json[REFERENCE]["accession"]
    version_tag = version["tag"]
    versions_dir = f"datasets/{dataset_name}/{REFERENCES}/{ref_accession}/versions"
    files_dir = f"{versions_dir}/{version_tag}/files"
    filenames = version['files'].copy()
    filenames["tag"] = "tag.json"
    files = {f"{filetype}": f"/{files_dir}/{filename}" for filetype, filename in filenames.items()}

    input_files_dir_abs = os.path.realpath(os.path.join(DATA_INPUT_DIR, files_dir))
    output_files_dir_abs = os.path.realpath(os.path.join(DATA_OUTPUT_DIR, files_dir))

    zip_dir = f"{versions_dir}/{version_tag}/zip-bundle"
    zip_base = f"nextclade_dataset_{dataset_name}_{ref_accession}_{version_tag}"
    zip_filename = f"{zip_base}.zip"
    zip_bundle_url = f"/{zip_dir}/{zip_filename}"

    zip_base_path = os.path.join(DATA_OUTPUT_DIR, zip_dir, zip_base)
    zip_src_dir = os.path.realpath(os.path.join(DATA_OUTPUT_DIR, files_dir))

    return dict_to_namedtuple("paths", {
        "files": files,
        "versions_dir": versions_dir,
        "input_files_dir_abs": input_files_dir_abs,
        "output_files_dir_abs": output_files_dir_abs,
        "zip_base_path": zip_base_path,
        "zip_src_dir": zip_src_dir,
        "zip_bundle_url": zip_bundle_url
    })


def copy_dataset_version_files(version, src_dir, dst_dir):
    os.makedirs(dst_dir, exist_ok=True)
    for _, filename in version['files'].items():
        input_filepath = os.path.join(src_dir, filename)
        output_filepath = os.path.join(dst_dir, filename)
        shutil.copy2(input_filepath, output_filepath)


def make_zip_bundle(dataset_json, dataset_ref_json, version):
    paths = get_paths(dataset_json, dataset_ref_json, version)
    os.makedirs(os.path.dirname(paths.zip_base_path), exist_ok=True)
    shutil.make_archive(
        base_name=paths.zip_base_path,
        format='zip',
        root_dir=paths.zip_src_dir
    )


if __name__ == '__main__':
    shutil.rmtree(DATA_OUTPUT_DIR, ignore_errors=True)

    with open(SETTINGS_JSON_PATH, 'r') as f:
        settings_json = json.load(f)

    settings = settings_json
    defaultDatasetName = settings['defaultDatasetName']
    defaultDatasetNameFriendly = None

    datasets = []
    for dataset_json_path in find_files(pattern="dataset.json", here=os.path.join(DATA_INPUT_DIR, "datasets")):
        with open(dataset_json_path, 'r') as f:
            dataset_json: dict = json.load(f)
        dataset_json_original = dataset_json.copy()

        dataset_name = dataset_json["name"]
        if dataset_name == defaultDatasetName:
            defaultDatasetNameFriendly = dataset_json['nameFriendly']

        dataset_dir = os.path.dirname(dataset_json_path)
        dataset_refs: List[dict] = []
        for dataset_ref_json_path in find_files(pattern="datasetRef.json", here=dataset_dir):

            with open(dataset_ref_json_path, 'r') as f:
                dataset_ref_json: dict = json.load(f)

            dataset_ref_dir = os.path.dirname(dataset_ref_json_path)

            # Read data descriptions for the tags
            version_jsons: List[dict] = []
            for tag_path in find_files("tag.json", dataset_ref_dir):
                with open(tag_path, 'r') as f:
                    version_json: dict = json.load(f)
                version_jsons.append(version_json)

            del version_json
            version_jsons.sort(key=lambda x: x["tag"], reverse=True)

            for i, version_json in enumerate(version_jsons):
                tag = version_json["tag"]
                paths = get_paths(dataset_json, dataset_ref_json, version_json)

                # Generate `tag.json` inside output directory
                tag_metadata = {**version_json["metadata"], **dataset_json["metadata"], **dataset_ref_json["metadata"]}
                tag_json = {**version_json, **dataset_json, **dataset_ref_json, "metadata": tag_metadata}
                tag_json_path = os.path.join(paths.output_files_dir_abs, "tag.json")
                json_write(tag_json, tag_json_path)

                # Copy files, including `tag.json` into output directory
                copy_dataset_version_files(version_json, src_dir=paths.input_files_dir_abs,
                                           dst_dir=paths.output_files_dir_abs)

                # Zip output directory
                make_zip_bundle(dataset_json, dataset_ref_json, version_json)

                # Copy latest version output directory to directory `latest`
                # (assumes `version_jsons` are sorted by tag, reversed!)
                if i == 0:
                    this_version_dir = os.path.join(DATA_OUTPUT_DIR, paths.versions_dir, tag)
                    latest_version_dir = os.path.join(DATA_OUTPUT_DIR, paths.versions_dir, "latest")
                    shutil.copytree(this_version_dir, latest_version_dir)

                version_json.update({"files": paths.files, "zipBundle": paths.zip_bundle_url, "latest": i == 0})

            for version_json in version_jsons:
                dataset_refs.append({**dataset_ref_json, **version_json})


        for dataset_ref in dataset_refs:
            # HACK: converts from old hierarchical object format to flat attributes
            # TODO: decide how to reorganize the hierarchical objects and folder structure in the git repo and rewrite this
            attributes = {
                "name": { "value": dataset_name, "isDefault": dataset_name == defaultDatasetName },
                "nameFriendly": {"value": dataset_json["nameFriendly"], "isDefault": dataset_json["nameFriendly"] == defaultDatasetNameFriendly },
                "reference": { "value":  dataset_ref["reference"]["accession"], "isDefault": dataset_ref["reference"]["accession"] == dataset_json["defaultRef"] },
                "tag": { "value": dataset_ref["tag"], "isDefault": dataset_ref["latest"] },
            }

            # TODO: figure out where 'enabled' field should come from
            dataset = {**dataset_json, **dataset_ref, "attributes": attributes}

            # HACK: converts files to the new format
            # TODO: modify original dataset files instead
            def convert_files(files):
                def get_filename(url):
                    return url.split("/")[-1]

                def remove_leading_slash(url):
                    return url.lstrip("/")

                return {get_filename(url): remove_leading_slash(url) for _, url in files.items()}

            dataset["files"] = convert_files(dataset["files"])

            # HACK: omits properties that have been moved into the attributes just above
            dataset = {k: v for k, v in dataset.items() if k in [
                "enabled",
                "attributes",
                "comment",
                "compatibility",
                "defaultGene",
                "files",
                "geneOrderPreference",
                "zipBundle",
            ]}

            datasets.append(dataset)

    datasets.sort(key=lambda x: x["attributes"]["name"]["value"])
    datasets.sort(key=lambda x: x["attributes"]["name"]["value"].startswith("sars-cov-2"), reverse=True)

    index_json = dict()
    index_json.update({"schema": "2.0.0"})
    index_json.update({"datasets": datasets})
    json_write(index_json, INDEX_JSON_PATH)
