#!/usr/bin/env python3

"""
Releases a new version of a dataset
"""
import logging
from os.path import dirname, realpath, join, relpath

from lib.changelog import changelog_prepare
from lib.container import dict_get, dict_get_required, dict_set, unique, find_index_by, first
from lib.fs import json_read, find_files, json_write, file_write, copy, make_zip
from lib.date import now_iso, iso_to_iso_safe
from lib.git import git_get_modified_files, git_dir_is_clean, git_get_dirty_files, git_check_tag, \
  git_get_initial_commit_hash, git_commit_all, git_push, github_create_release, git_pull

REPO = "neherlab/nextclade_data"
THIS_DIR = dirname(realpath(__file__))
PROJECT_ROOT_DIR = realpath(join(THIS_DIR, ".."))
DATA_INPUT_DIR = realpath(join(PROJECT_ROOT_DIR, "data_v3"))
DATA_OUTPUT_DIR = realpath(join(PROJECT_ROOT_DIR, "data_v3_output"))

logging.basicConfig(level=logging.INFO)
l = logging.getLogger(__file__)


def prepare_dataset_release_info(dataset_dir, dataset, last_version, updated_at):
  modified_files = list(git_get_modified_files(from_revision=last_version, dirs=dataset_dir))

  if len(modified_files) == 0:
    return None

  path = dataset["path"]
  l.info(f"Preparing release of '{path}'")

  changelog_path = relpath(join(dataset_dir, "CHANGELOG.md"), PROJECT_ROOT_DIR)

  if changelog_path not in modified_files:
    raise ValueError(
      f"Cannot release dataset '{path}' without changelog. Please add or modify file '{changelog_path}': add '## Unreleased' section and briefly summarize the changes being released"
    )

  release_notes = changelog_prepare(dataset, updated_at, changelog_path)

  return {"dataset": dataset, "release_notes": release_notes, "dataset_dir": dataset_dir}


def get_dataset_name(dataset):
  return dict_get(dataset, ["attributes", "name", "valueFriendly"]) \
    or dict_get_required(dataset, ["attributes", "name", "value"])


def main():
  if not git_dir_is_clean():
    dirty_files = "\n  ".join(git_get_dirty_files())
    raise ValueError(
      f"Uncommited changes detected. Refusing to proceed. Commit or stash changes first:\n  {dirty_files}"
    )

  git_pull()

  index_json_path = join(DATA_INPUT_DIR, "index.json")
  index_json = json_read(index_json_path)
  datasets = dict_get(index_json, ["datasets"])

  updated_at = now_iso()
  tag = iso_to_iso_safe(updated_at)
  git_check_tag(tag)

  release_infos = []
  for pathogen_json_path in find_files("pathogen.json", DATA_INPUT_DIR):
    dataset_dir = dirname(pathogen_json_path)
    dataset_dir_rel = relpath(dataset_dir, DATA_INPUT_DIR)

    i_dataset = find_index_by(lambda dataset: dataset["path"] == dataset_dir_rel, datasets)
    if i_dataset is None:
      raise ValueError(
        f"Dataset at '{dataset_dir_rel}' not found in the dataset index ({index_json_path}). Try to reindex datasets first.")

    dataset = datasets[i_dataset]
    versions = dict_get(dataset, ["versions"]) or []
    last_version = first(sorted(versions, reverse=True)) or git_get_initial_commit_hash()

    release_info = prepare_dataset_release_info(dataset_dir, dataset, last_version, updated_at)
    if release_info is None:
      continue

    release_infos.append(release_info)

    versions.insert(0, tag)
    dict_set(dataset, ["versions"], versions)
    dict_set(dataset, ["updatedAt"], updated_at)

    pathogen_json = json_read(pathogen_json_path)
    dict_set(pathogen_json, ["updatedAt"], updated_at)
    json_write(pathogen_json, pathogen_json_path, no_sort_keys=True)

    create_dataset_package(dataset, tag, dataset_dir)

  if len(release_infos) == 0:
    l.info("No dataset modifications detected. Will not release anything.")
    return

  json_write({**index_json, "datasets": datasets, "updatedAt": updated_at}, index_json_path, no_sort_keys=True)

  dataset_names = ", ".join(unique([
    get_dataset_name(release_info["dataset"])
    for release_info in release_infos
  ]))

  release_notes = f"This release contains changes for datasets: {dataset_names}\n\n\n"
  for release_info in release_infos:
    release_notes += f'\n{release_info["release_notes"]}\n\n'

  create_release_package(release_notes)
  release_files = list(find_files("*", join(DATA_OUTPUT_DIR, "github_releases")))

  l.info(f"Commiting changes for '{tag}'")
  commit_hash = git_commit_all(f"chore: release '{tag}'")
  l.info(f"Releasing '{tag}' (commit: '{commit_hash}')")
  git_push()
  github_create_release(
    repo=REPO,
    version=tag,
    commit_hash=commit_hash,
    release_notes=release_notes,
    files=release_files
  )


def create_release_package(release_notes):
  copy(join(DATA_INPUT_DIR, "index.json"), join(DATA_OUTPUT_DIR, "server/"))
  file_write(release_notes, join(DATA_OUTPUT_DIR, "github_releases", "CHANGELOG.md"))


def create_dataset_package(dataset, tag, dataset_dir):
  path = dataset["path"]
  files = dataset["capabilities"]["files"]
  out_dir = join(DATA_OUTPUT_DIR, "server", path, tag)
  for _, file in files.items():
    file = join(dataset_dir, file)
    copy(file, f"{out_dir}/")  # trailing slash is required if destination is a directory

  zip_basename = join(out_dir, "dataset")
  make_zip(dataset_dir, zip_basename)

  path_safe = path.replace("/", "__")
  copy(f"{zip_basename}.zip", join(DATA_OUTPUT_DIR, "github_releases", f"{path_safe}__{tag}.zip"))


if __name__ == '__main__':
  main()
